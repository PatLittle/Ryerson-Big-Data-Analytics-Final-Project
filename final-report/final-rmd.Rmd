---
title: "final-rmd"
output:
  html_document:
    df_print: paged
---

```{r}
library(readxl)
library(openxlsx)
library(tidyverse)
library(R.utils)
library(tidymodels)
library(jsonlite)
library(gtools)
library(ggpubr)
library(plyr)
library(dplyr)
library(tictoc)
library(tidymodels)
library(data.table)
library(R.oo)
library(tibble)
```


```{r cache=TRUE}

tic()
wb<-loadWorkbook("downloads-012020-012021.xlsx")
removeWorksheet(wb, 1)#remove the unwanted tabs
removeWorksheet(wb, 1)#do it again for the 2nd unwanted
saveWorkbook(wb,"downloads.xlsx", overwrite = T)

path<-"downloads.xlsx"
dls<-lapply(excel_sheets(path), read_excel, path = path)

dl_df<-map_dfr(dls,`[`, c("ID / Identificateur","Title English / Titre en anglais","Number of downloads / Nombre de téléchargements"))
dl_df<-na.omit(dl_df)
dl_df$`Title English / Titre en anglais`<-NULL
names(dl_df)<-c("ID","downloads")


R.utils::gunzip("od-do-canada.jsonl.gz", remove=F)
query1<-readLines("od-do-canada.jsonl")
lines <- lapply(query1,unlist)


q1<-fromJSON(lines[[1]])
ID<-q1$id
org<-q1$organization$name
desc<-as.numeric(length(unlist(strsplit(q1$notes," "))))
collection<-q1$collection
freq<-q1$frequency
jurisdiction<-q1$jurisdiction
key1<-q1$keywords$en[1]
key2<-q1$keywords$en[2]
key3<-q1$keywords$en[3]
num_keys<-as.numeric(length(q1$keywords$en))
num_res<-as.numeric(q1$num_resources)
subj1<-q1$subject[1]
subj2<-q1$subject[2]
subj3<-q1$subject[3]
subj4<-q1$subject[4]
date_created<-q1$metadata_created
date_last_mod<-q1$metadata_modified
q1data<-data.frame(ID,org,desc,collection,freq,jurisdiction,key1,key2,key3,num_keys,num_res,subj1,subj2,subj3,subj4,date_created,date_last_mod)
names(q1data)<-c("ID","org","desc","collection","freq","jurisdiction","key1","key2","key3","num_keys","num_res","subj1","subj2","subj3","subj4","date_created","date_last_mod")

for(i in 2:length(lines)){ #loop over this for each line of json - except the 1st line
  q1<-fromJSON(lines[[i]])
  ID<-q1$id
  org<-q1$organization$name
  desc<-as.numeric(length(unlist(strsplit(q1$notes," "))))
  collection<-q1$collection
  freq<-q1$frequency
  jurisdiction<-q1$jurisdiction
  key1<-q1$keywords$en[1]
  key2<-q1$keywords$en[2]
  key3<-q1$keywords$en[3]
  num_keys<-as.numeric(length(q1$keywords$en))
  num_res<-as.numeric(q1$num_resources)
  subj1<-q1$subject[1]
  subj2<-q1$subject[2]
  subj3<-q1$subject[3]
  subj4<-q1$subject[4]
  date_created<-q1$metadata_created
  date_last_mod<-q1$metadata_modified
  q1data<- q1data %>% add_row(ID,org,desc,collection,freq,jurisdiction,key1,key2,key3,num_keys,num_res,subj1,subj2,subj3,subj4,date_created,date_last_mod)
}



combined<-merge(x = q1data, y = dl_df, by = "ID", all.x = TRUE)
combined<-na.replace(combined,0)


combined$date_created<-as.Date(combined$date_created)
combined$date_last_mod<-as.Date(combined$date_last_mod)

date_of_downloads<-as.Date.character("2021-01-31","%Y-%m-%d")
combined$created_days<-as.numeric(difftime(date_of_downloads,combined$date_created, units="days"))
combined$modified_days<-as.numeric(difftime(date_of_downloads,combined$date_last_mod, units="days"))

j<-1
for(i in 1:length(combined$downloads)){
  if (combined$created_days[i]< 365){
    combined$adj_downloads[j]<-round(365*(combined$downloads[j]/combined$created_days[j]),digits=0)
  } else combined$adj_downloads[j]<-combined$downloads[j]
  j<-j+1
}

quant_list<-as.list(quantile(combined$adj_downloads, probs = seq(0, 1, by= 0.01)))

quant_list[96]

j<-1
for(i in 1:length(combined$adj_downloads)){
  if (combined$adj_downloads[j]< quant_list[96]){
    combined$bin_downloads[j]<-0
  } else {
    combined$bin_downloads[j]<-1
  }
  j<-j+1
}

combined$bin_downloads<-as.factor(combined$bin_downloads)




df<-combined

df$ID<-NULL
df$date_created<-NULL
df$date_last_mod<-NULL
df$downloads<-NULL
df$created_days<-NULL
df$modified_days<-NULL
df$adj_downloads<-NULL

df<-as.data.table(df,keep.rownames = F)
df<-subset(df, collection!="geogratis")
df$key1<-str_to_lower(df$key1)
df$key2<-str_to_lower(df$key2)
df$key3<-str_to_lower(df$key3)
df$key1<-str_replace_all(df$key1, regex("\\W+"),"")
df$key2<-str_replace_all(df$key2, regex("\\W+"),"")
df$key3<-str_replace_all(df$key3, regex("\\W+"),"")

df<-df
df$org<-as.numeric(as.factor(df$org))
df$collection<-as.numeric(as.factor(df$collection))
df$freq<-as.numeric(as.factor(df$freq))
df$jurisdiction<-as.numeric(as.factor(df$jurisdiction))
df$key1<-as.numeric(as.factor(df$key1))
df$key2<-as.numeric(as.factor(df$key2))
df$key3<-as.numeric(as.factor(df$key3))
df$subj1<-as.numeric(as.factor(df$subj1))
df$subj2<-as.numeric(as.factor(df$subj2))
df$subj3<-as.numeric(as.factor(df$subj3))
df$subj4<-as.numeric(as.factor(df$subj4))
toc()
```

```{r cache=TRUE}
set.seed(888)

pop_split3<- initial_split(df,strata = bin_downloads)
pop_train3<-training(pop_split3)
pop_test3<-testing(pop_split3)

xgb_spec3 <- boost_tree(
  trees = 2000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), mtry = tune(),         
  learn_rate = tune()                         
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec3

xgb_grid3 <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), pop_train3),
  learn_rate(),
  size = 50
)

xgb_grid3

xgb_wf3 <- workflow() %>%
  add_formula(bin_downloads ~ .) %>%
  add_model(xgb_spec3)


xgb_wf3

pop_folds3<-vfold_cv(pop_train3,v=10,strata=bin_downloads)
pop_folds3


library(doParallel)
cores<-detectCores()
cl<- makeCluster(cores[1]-4)
registerDoParallel(cl)

tic()
set.seed(888)
xgb_res3 <- tune_grid(
  xgb_wf3,
  resamples = pop_folds3,
  grid = xgb_grid3,
  control = control_grid(save_pred = TRUE)
)
toc()

collect_metrics(xgb_res3)

best_auc <- select_best(xgb_res3, "roc_auc")
best_auc


final_xgb3 <- finalize_workflow(
  xgb_wf3,
  best_auc
)
final_xgb3

final_res3 <- last_fit(final_xgb3, pop_split3)
collect_metrics(final_res3)
final_res3


final_res3 %>%
  collect_predictions() %>%
  roc_curve(bin_downloads, .pred_0) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )

final_res3 %>%
  collect_predictions() %>% 
  conf_mat(truth = bin_downloads, estimate = .pred_class)




saveRDS(final_res3$.workflow[[1]],"final_model.Rds")
```
```{r cache=TRUE}
best_auc

library(vip)

final_xgb3 %>%
  fit(data = pop_train3) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

```{r cache=TRUE}
collect_metrics(xgb_res3)


xgb_res3 %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

collect_metrics(xgb_res3)


final_res3 %>%
  collect_predictions() %>% 
  conf_mat(truth = bin_downloads, estimate = .pred_class)
```

```{r}
final_res3 %>%
  collect_predictions() %>%
  roc_curve(bin_downloads, .pred_0) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```


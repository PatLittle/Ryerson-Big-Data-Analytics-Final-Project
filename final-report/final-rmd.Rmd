---
title: "final-rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

```{r}
library(readxl)
library(openxlsx)
library(tidyverse)
library(R.utils)
library(tidymodels)
library(jsonlite)
library(gtools)
library(ggpubr)
library(plyr)
library(dplyr)
library(tictoc)
library(tidymodels)
library(data.table)
library(R.oo)
library(tibble)
```


```{r cache=TRUE}

wb<-loadWorkbook("downloads-012020-012021.xlsx")
removeWorksheet(wb, 1)#remove the unwanted tabs
removeWorksheet(wb, 1)#do it again for the 2nd unwanted
saveWorkbook(wb,"downloads.xlsx", overwrite = T)

path<-"downloads.xlsx"
dls<-lapply(excel_sheets(path), read_excel, path = path)

dl_df<-map_dfr(dls,`[`, c("ID / Identificateur","Title English / Titre en anglais","Number of downloads / Nombre de téléchargements"))
dl_df<-na.omit(dl_df)
dl_df$`Title English / Titre en anglais`<-NULL
names(dl_df)<-c("ID","downloads")


R.utils::gunzip("od-do-canada.jsonl.gz", remove=F)
query1<-readLines("od-do-canada.jsonl")
lines <- lapply(query1,unlist)


q1<-fromJSON(lines[[1]])
ID<-q1$id
org<-q1$organization$name
desc<-as.numeric(length(unlist(strsplit(q1$notes," "))))
collection<-q1$collection
freq<-q1$frequency
jurisdiction<-q1$jurisdiction
key1<-q1$keywords$en[1]
key2<-q1$keywords$en[2]
key3<-q1$keywords$en[3]
num_keys<-as.numeric(length(q1$keywords$en))
num_res<-as.numeric(q1$num_resources)
subj1<-q1$subject[1]
subj2<-q1$subject[2]
subj3<-q1$subject[3]
subj4<-q1$subject[4]
date_created<-q1$metadata_created
date_last_mod<-q1$metadata_modified
q1data<-data.frame(ID,org,desc,collection,freq,jurisdiction,key1,key2,key3,num_keys,num_res,subj1,subj2,subj3,subj4,date_created,date_last_mod)
names(q1data)<-c("ID","org","desc","collection","freq","jurisdiction","key1","key2","key3","num_keys","num_res","subj1","subj2","subj3","subj4","date_created","date_last_mod")

for(i in 2:length(lines)){ #loop over this for each line of json - except the 1st line
  q1<-fromJSON(lines[[i]])
  ID<-q1$id
  org<-q1$organization$name
  desc<-as.numeric(length(unlist(strsplit(q1$notes," "))))
  collection<-q1$collection
  freq<-q1$frequency
  jurisdiction<-q1$jurisdiction
  key1<-q1$keywords$en[1]
  key2<-q1$keywords$en[2]
  key3<-q1$keywords$en[3]
  num_keys<-as.numeric(length(q1$keywords$en))
  num_res<-as.numeric(q1$num_resources)
  subj1<-q1$subject[1]
  subj2<-q1$subject[2]
  subj3<-q1$subject[3]
  subj4<-q1$subject[4]
  date_created<-q1$metadata_created
  date_last_mod<-q1$metadata_modified
  q1data<- q1data %>% add_row(ID,org,desc,collection,freq,jurisdiction,key1,key2,key3,num_keys,num_res,subj1,subj2,subj3,subj4,date_created,date_last_mod)
}



combined<-merge(x = q1data, y = dl_df, by = "ID", all.x = TRUE)
combined<-na.replace(combined,0)


combined$date_created<-as.Date(combined$date_created)
combined$date_last_mod<-as.Date(combined$date_last_mod)

##################start removing combined_factor####################


date_of_downloads<-as.Date.character("2021-01-31","%Y-%m-%d")
combined$created_days<-as.numeric(difftime(date_of_downloads,combined$date_created, units="days"))
combined$modified_days<-as.numeric(difftime(date_of_downloads,combined$date_last_mod, units="days"))

j<-1
for(i in 1:length(combined$downloads)){
  if (combined$created_days[i]< 365){
    combined$adj_downloads[j]<-round(365*(combined$downloads[j]/combined$created_days[j]),digits=0)
  } else combined$adj_downloads[j]<-combined$downloads[j]
  j<-j+1
}

quant_list<-as.list(quantile(combined$adj_downloads, probs = seq(0, 1, by= 0.01)))

j<-1
for(i in 1:length(combined$adj_downloads)){
  if (combined$adj_downloads[j]< quant_list[96]){
    combined$bin_downloads[j]<-0
  } else {
    combined$bin_downloads[j]<-1
  }
  j<-j+1
}

combined$bin_downloads<-as.factor(combined$bin_downloads)




df<-combined

df$ID<-NULL
df$date_created<-NULL
df$date_last_mod<-NULL
df$downloads<-NULL
df$created_days<-NULL
df$modified_days<-NULL
df$adj_downloads<-NULL

df<-as.data.table(df,keep.rownames = F)
df<-subset(df, collection!="geogratis")
df$key1<-str_to_lower(df$key1)
df$key2<-str_to_lower(df$key2)
df$key3<-str_to_lower(df$key3)
df$key1<-str_replace_all(df$key1, regex("\\W+"),"")
df$key2<-str_replace_all(df$key2, regex("\\W+"),"")
df$key3<-str_replace_all(df$key3, regex("\\W+"),"")

df_2<-df
df_2$org<-as.numeric(as.factor(df_2$org))
df_2$collection<-as.numeric(as.factor(df_2$collection))
df_2$freq<-as.numeric(as.factor(df_2$freq))
df_2$jurisdiction<-as.numeric(as.factor(df_2$jurisdiction))
df_2$key1<-as.numeric(as.factor(df_2$key1))
df_2$key2<-as.numeric(as.factor(df_2$key2))
df_2$key3<-as.numeric(as.factor(df_2$key3))
df_2$subj1<-as.numeric(as.factor(df_2$subj1))
df_2$subj2<-as.numeric(as.factor(df_2$subj2))
df_2$subj3<-as.numeric(as.factor(df_2$subj3))
df_2$subj4<-as.numeric(as.factor(df_2$subj4))

##########build model##############


```


```{r cache=TRUE}
set.seed(888)
pop_split<- initial_split(df,strata = bin_downloads)
pop_train<-training(pop_split)
pop_test<-testing(pop_split)

set.seed(888)
pop_folds<-vfold_cv(pop_train,v=10,strata=bin_downloads)

xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), mtry = tune(),         
  learn_rate = tune()                        
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), pop_train),
  learn_rate(),
  size = 35
)

xgb_grid

xgb_recipe <- recipe(bin_downloads ~ ., data = pop_train) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())
  


tic()
xgb_recipe %>%
  prep() %>%
  bake(new_data = pop_train) 
toc()


xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_recipe)

xgb_wf



```


```{r cache=TRUE}
```


```{r cache=TRUE}
library(doParallel)
cores<-detectCores()
cl<- makeCluster(cores[1]-4)
registerDoParallel(cl)

tic()
set.seed(888)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = pop_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE))
toc()



best_auc <- select_best(xgb_res, "roc_auc")
best_auc


final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)
```


```{r cache=TRUE}
```


```{r cache=TRUE}
tic()
final_res <- last_fit(final_xgb, pop_split)
collect_metrics(final_res)
toc()

final_res %>%
  collect_predictions() %>%
  roc_curve(bin_downloads, .pred_0) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )


final_res %>%
  collect_predictions() %>% 
  conf_mat(truth = bin_downloads, estimate = .pred_class)

library(vip)
final_xgb %>%
  fit(data = pop_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")

final_res$.workflow[[1]]
saveRDS(final_res$.workflow[[1]],"saved_model.Rds")

saved_model<-readRDS("saved_model.Rds")

predict(saved_model,pop_test[2,])
```
```{r cache=TRUE}

set.seed(888)

pop_split2<- initial_split(df_2,strata = bin_downloads)
pop_train2<-training(pop_split2)
pop_test2<-testing(pop_split2)

xgb_spec2 <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune()                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec2

xgb_grid2 <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), pop_train2),
  learn_rate(),
  size = 35
)

xgb_grid2

xgb_wf2 <- workflow() %>%
  add_formula(bin_downloads ~ .) %>%
  add_model(xgb_spec2)


xgb_wf2

pop_folds2<-vfold_cv(pop_train2,v=10,strata=bin_downloads)
pop_folds2



```

```{r cache=TRUE}
library(doParallel)
cores<-detectCores()
cl<- makeCluster(cores[1]-4)
registerDoParallel(cl)

tic()
set.seed(888)
xgb_res2 <- tune_grid(
  xgb_wf2,
  resamples = pop_folds2,
  grid = xgb_grid2,
  control = control_grid(save_pred = TRUE)
)
tic()

collect_metrics(xgb_res2)

best_auc <- select_best(xgb_res2, "roc_auc")
best_auc


final_xgb2 <- finalize_workflow(
  xgb_wf2,
  best_auc
)

final_xgb2

library(vip)
install.packages("vip")
final_xgb2 %>%
  fit(data = pop_train2) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")


final_res2 <- last_fit(final_xgb2, pop_split2)
collect_metrics(final_res2)

final_res2 %>%
  collect_predictions() %>%
  roc_curve(bin_downloads, .pred_0) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )

final_res2 %>%
  collect_predictions() %>% 
  conf_mat(truth = bin_downloads, estimate = .pred_class)




saveRDS(final_res2$.workflow[[1]],"saved_model2.Rds")

saved_model2<-readRDS("saved_model2.Rds")

```

```{r}
set.seed(888)

pop_split3<- initial_split(df_2,strata = bin_downloads)
pop_train3<-training(pop_split3)
pop_test3<-testing(pop_split3)

xgb_spec3 <- boost_tree(
  trees = 2000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune()                         ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec3

xgb_grid3 <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), pop_train2),
  learn_rate(),
  size = 50
)

xgb_grid3

xgb_wf3 <- workflow() %>%
  add_formula(bin_downloads ~ .) %>%
  add_model(xgb_spec3)


xgb_wf3

pop_folds3<-vfold_cv(pop_train3,v=10,strata=bin_downloads)
pop_folds3


library(doParallel)
cores<-detectCores()
cl<- makeCluster(cores[1]-4)
registerDoParallel(cl)

tic()
set.seed(888)
xgb_res3 <- tune_grid(
  xgb_wf3,
  resamples = pop_folds3,
  grid = xgb_grid3,
  control = control_grid(save_pred = TRUE)
)
tic()

collect_metrics(xgb_res3)

best_auc <- select_best(xgb_res3, "roc_auc")
best_auc


final_xgb3 <- finalize_workflow(
  xgb_wf3,
  best_auc
)

final_xgb3

library(vip)

final_xgb3 %>%
  fit(data = pop_train3) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")


final_res3 <- last_fit(final_xgb3, pop_split3)
collect_metrics(final_res2)

final_res3 %>%
  collect_predictions() %>%
  roc_curve(bin_downloads, .pred_0) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )

final_res3 %>%
  collect_predictions() %>% 
  conf_mat(truth = bin_downloads, estimate = .pred_class)




saveRDS(final_res3$.workflow[[1]],"saved_model3.Rds")

saved_model3<-readRDS("saved_model3.Rds")

```

